#!/usr/bin/env python3
"""
OpenAI API Benchmark Test
æµ‹è¯•ä¸åŒæ–‡æœ¬é•¿åº¦å’Œè¯­éŸ³è®¾ç½®ä¸‹çš„æ€§èƒ½
"""
import requests
import time
import json
from pathlib import Path
import statistics

BASE_URL = "https://voxcpm-tts.aws.xin"
OUTPUT_DIR = Path("benchmark_openai_results")
OUTPUT_DIR.mkdir(exist_ok=True)

# æµ‹è¯•æ–‡æœ¬
TEST_TEXTS = {
    "short": "ä½ å¥½ï¼Œè¿™æ˜¯ä¸€ä¸ªç®€çŸ­çš„æµ‹è¯•ã€‚",
    "medium": "äººå·¥æ™ºèƒ½æŠ€æœ¯æ­£åœ¨å¿«é€Ÿå‘å±•ï¼Œè¯­éŸ³åˆæˆä½œä¸ºå…¶ä¸­çš„é‡è¦åˆ†æ”¯ï¼Œå·²ç»åœ¨å¤šä¸ªé¢†åŸŸå¾—åˆ°äº†å¹¿æ³›åº”ç”¨ã€‚VoxCPM æä¾›äº†é«˜è´¨é‡çš„è¯­éŸ³åˆæˆæœåŠ¡ã€‚",
    "long": "åœ¨å½“ä»Šæ•°å­—åŒ–æ—¶ä»£ï¼Œäººå·¥æ™ºèƒ½æŠ€æœ¯çš„å‘å±•æ—¥æ–°æœˆå¼‚ï¼Œå…¶ä¸­è¯­éŸ³åˆæˆæŠ€æœ¯ä½œä¸ºäººæœºäº¤äº’çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œæ­£åœ¨ç»å†ç€é©å‘½æ€§çš„å˜é©ã€‚ä»æ—©æœŸçš„æœºæ¢°å¼åˆæˆåˆ°ç°åœ¨çš„ç¥ç»ç½‘ç»œé©±åŠ¨çš„è‡ªç„¶è¯­éŸ³ç”Ÿæˆï¼ŒæŠ€æœ¯çš„è¿›æ­¥è®©æœºå™¨çš„å£°éŸ³è¶Šæ¥è¶Šæ¥è¿‘çœŸäººã€‚VoxCPM ä½œä¸ºæ–°ä¸€ä»£çš„è¯­éŸ³åˆæˆç³»ç»Ÿï¼Œé‡‡ç”¨äº†å…ˆè¿›çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡ã€è‡ªç„¶æµç•…çš„è¯­éŸ³ã€‚æ— è®ºæ˜¯åœ¨æ™ºèƒ½å®¢æœã€æœ‰å£°è¯»ç‰©ã€è¿˜æ˜¯è¾…åŠ©æŠ€æœ¯é¢†åŸŸï¼Œè¯­éŸ³åˆæˆéƒ½å±•ç°å‡ºäº†å·¨å¤§çš„åº”ç”¨ä»·å€¼ã€‚"
}

def upload_voice_and_get_id(audio_file_path):
    """ä¸Šä¼ è¯­éŸ³æ–‡ä»¶å¹¶è·å– voice_idï¼ˆæ¨¡æ‹Ÿé¢„è®¾è¯­éŸ³ï¼‰"""
    # æ³¨æ„ï¼šå½“å‰ OpenAI API ä¸æ”¯æŒä¸Šä¼ ï¼Œæˆ‘ä»¬ä½¿ç”¨é¢„è®¾çš„ voice_id
    # è¿™é‡Œè¿”å›é»˜è®¤çš„ voice_id
    return "default"

def test_openai_api(text, voice="alloy", model="tts-1", format="mp3", run_num=1):
    """æµ‹è¯• OpenAI API"""
    print(f"\n{'='*60}")
    print(f"Run #{run_num}: {len(text)} chars, voice={voice}, model={model}")
    print(f"{'='*60}")
    
    url = f"{BASE_URL}/v1/audio/speech"
    payload = {
        "model": model,
        "input": text,
        "voice": voice,
        "response_format": format
    }
    
    start_time = time.time()
    first_byte_time = None
    total_bytes = 0
    
    try:
        response = requests.post(url, json=payload, stream=True, timeout=120)
        
        if response.status_code != 200:
            print(f"âŒ Error: {response.status_code}")
            print(f"   {response.text}")
            return None
        
        # æ¥æ”¶æµå¼æ•°æ®
        for chunk in response.iter_content(chunk_size=8192):
            if chunk:
                if first_byte_time is None:
                    first_byte_time = time.time()
                    first_byte_latency = first_byte_time - start_time
                    print(f"â±ï¸  é¦–å­—èŠ‚å»¶è¿Ÿ: {first_byte_latency:.3f}s")
                total_bytes += len(chunk)
        
        total_time = time.time() - start_time
        
        result = {
            "text_length": len(text),
            "voice": voice,
            "model": model,
            "format": format,
            "first_byte_latency": first_byte_latency,
            "total_time": total_time,
            "total_bytes": total_bytes,
            "throughput_kbps": (total_bytes * 8 / 1024) / total_time if total_time > 0 else 0
        }
        
        print(f"âœ… å®Œæˆ!")
        print(f"   é¦–å­—èŠ‚: {first_byte_latency:.3f}s")
        print(f"   æ€»æ—¶é—´: {total_time:.3f}s")
        print(f"   æ–‡ä»¶å¤§å°: {total_bytes/1024:.1f} KB")
        print(f"   ååé‡: {result['throughput_kbps']:.1f} Kbps")
        
        return result
        
    except Exception as e:
        print(f"âŒ å¼‚å¸¸: {e}")
        return None

def test_native_api_with_voice_id(text, voice_id="default", run_num=1):
    """æµ‹è¯•åŸç”Ÿ API ä½¿ç”¨ voice_id"""
    print(f"\n{'='*60}")
    print(f"Run #{run_num} (Native API): {len(text)} chars, voice_id={voice_id}")
    print(f"{'='*60}")
    
    url = f"{BASE_URL}/api/tts/stream"
    
    data = {
        "text": text,
        "voice_id": voice_id,
        "cfg_value": "2.0",
        "inference_timesteps": "5"
    }
    
    start_time = time.time()
    first_byte_time = None
    total_bytes = 0
    
    try:
        response = requests.post(url, data=data, stream=True, timeout=120)
        
        if response.status_code != 200:
            print(f"âŒ Error: {response.status_code}")
            return None
        
        for chunk in response.iter_content(chunk_size=8192):
            if chunk:
                if first_byte_time is None:
                    first_byte_time = time.time()
                    first_byte_latency = first_byte_time - start_time
                    print(f"â±ï¸  é¦–å­—èŠ‚å»¶è¿Ÿ: {first_byte_latency:.3f}s")
                total_bytes += len(chunk)
        
        total_time = time.time() - start_time
        
        result = {
            "text_length": len(text),
            "voice_id": voice_id,
            "api_type": "native",
            "first_byte_latency": first_byte_latency,
            "total_time": total_time,
            "total_bytes": total_bytes,
            "throughput_kbps": (total_bytes * 8 / 1024) / total_time if total_time > 0 else 0
        }
        
        print(f"âœ… å®Œæˆ!")
        print(f"   é¦–å­—èŠ‚: {first_byte_latency:.3f}s")
        print(f"   æ€»æ—¶é—´: {total_time:.3f}s")
        print(f"   æ–‡ä»¶å¤§å°: {total_bytes/1024:.1f} KB")
        
        return result
        
    except Exception as e:
        print(f"âŒ å¼‚å¸¸: {e}")
        return None

def run_benchmark():
    """è¿è¡Œå®Œæ•´çš„ benchmark"""
    print("\n" + "ğŸ¯"*30)
    print("OpenAI API Benchmark Test")
    print("ğŸ¯"*30)
    
    all_results = []
    
    # æµ‹è¯•é…ç½®
    test_configs = [
        # OpenAI API æµ‹è¯•
        {"api": "openai", "text_type": "short", "model": "tts-1", "voice": "alloy", "runs": 3},
        {"api": "openai", "text_type": "medium", "model": "tts-1", "voice": "alloy", "runs": 3},
        {"api": "openai", "text_type": "long", "model": "tts-1", "voice": "alloy", "runs": 3},
        {"api": "openai", "text_type": "medium", "model": "tts-1-hd", "voice": "nova", "runs": 3},
        
        # Native API æµ‹è¯•ï¼ˆä½¿ç”¨ voice_idï¼‰
        {"api": "native", "text_type": "short", "voice_id": "default", "runs": 3},
        {"api": "native", "text_type": "medium", "voice_id": "default", "runs": 3},
        {"api": "native", "text_type": "long", "voice_id": "default", "runs": 3},
    ]
    
    for config in test_configs:
        text = TEST_TEXTS[config["text_type"]]
        runs = config["runs"]
        
        print(f"\n{'#'*60}")
        print(f"æµ‹è¯•é…ç½®: {config}")
        print(f"{'#'*60}")
        
        run_results = []
        
        for i in range(runs):
            time.sleep(2)  # é¿å…è¯·æ±‚è¿‡å¿«
            
            if config["api"] == "openai":
                result = test_openai_api(
                    text=text,
                    voice=config["voice"],
                    model=config["model"],
                    run_num=i+1
                )
            else:  # native
                result = test_native_api_with_voice_id(
                    text=text,
                    voice_id=config["voice_id"],
                    run_num=i+1
                )
            
            if result:
                result["config"] = config
                run_results.append(result)
        
        if run_results:
            # è®¡ç®—ç»Ÿè®¡æ•°æ®
            first_byte_latencies = [r["first_byte_latency"] for r in run_results]
            total_times = [r["total_time"] for r in run_results]
            
            summary = {
                "config": config,
                "runs": len(run_results),
                "first_byte_latency": {
                    "mean": statistics.mean(first_byte_latencies),
                    "min": min(first_byte_latencies),
                    "max": max(first_byte_latencies),
                    "stdev": statistics.stdev(first_byte_latencies) if len(first_byte_latencies) > 1 else 0
                },
                "total_time": {
                    "mean": statistics.mean(total_times),
                    "min": min(total_times),
                    "max": max(total_times),
                    "stdev": statistics.stdev(total_times) if len(total_times) > 1 else 0
                },
                "results": run_results
            }
            
            all_results.append(summary)
            
            print(f"\nğŸ“Š ç»Ÿè®¡ç»“æœ:")
            print(f"   é¦–å­—èŠ‚å»¶è¿Ÿ: {summary['first_byte_latency']['mean']:.3f}s Â± {summary['first_byte_latency']['stdev']:.3f}s")
            print(f"   æ€»æ—¶é—´: {summary['total_time']['mean']:.3f}s Â± {summary['total_time']['stdev']:.3f}s")
    
    # ä¿å­˜ç»“æœ
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    json_file = OUTPUT_DIR / f"benchmark_openai_{timestamp}.json"
    
    with open(json_file, "w", encoding="utf-8") as f:
        json.dump(all_results, f, indent=2, ensure_ascii=False)
    
    print(f"\nâœ… ç»“æœå·²ä¿å­˜åˆ°: {json_file}")
    
    # ç”Ÿæˆ Markdown æŠ¥å‘Š
    generate_report(all_results, timestamp)
    
    return all_results

def generate_report(results, timestamp):
    """ç”Ÿæˆ Markdown æŠ¥å‘Š"""
    md_file = OUTPUT_DIR / f"benchmark_openai_{timestamp}.md"
    
    with open(md_file, "w", encoding="utf-8") as f:
        f.write("# OpenAI API Benchmark Report\n\n")
        f.write(f"**æµ‹è¯•æ—¶é—´**: {time.strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        f.write(f"**æµ‹è¯•æœåŠ¡å™¨**: {BASE_URL}\n\n")
        
        f.write("## æµ‹è¯•é…ç½®\n\n")
        f.write("| API | æ–‡æœ¬ç±»å‹ | æ–‡æœ¬é•¿åº¦ | æ¨¡å‹/è¯­éŸ³ | è¿è¡Œæ¬¡æ•° |\n")
        f.write("|-----|---------|---------|----------|----------|\n")
        
        for result in results:
            config = result["config"]
            text_len = len(TEST_TEXTS[config["text_type"]])
            if config["api"] == "openai":
                model_voice = f"{config['model']}/{config['voice']}"
            else:
                model_voice = f"voice_id={config['voice_id']}"
            
            f.write(f"| {config['api']} | {config['text_type']} | {text_len} | {model_voice} | {result['runs']} |\n")
        
        f.write("\n## æ€§èƒ½ç»“æœ\n\n")
        f.write("| API | æ–‡æœ¬ç±»å‹ | é¦–å­—èŠ‚å»¶è¿Ÿ (s) | æ€»æ—¶é—´ (s) | æ–‡ä»¶å¤§å° (KB) |\n")
        f.write("|-----|---------|----------------|-----------|-------------|\n")
        
        for result in results:
            config = result["config"]
            fb_mean = result["first_byte_latency"]["mean"]
            fb_std = result["first_byte_latency"]["stdev"]
            tt_mean = result["total_time"]["mean"]
            tt_std = result["total_time"]["stdev"]
            
            avg_size = statistics.mean([r["total_bytes"]/1024 for r in result["results"]])
            
            f.write(f"| {config['api']} | {config['text_type']} | "
                   f"{fb_mean:.3f} Â± {fb_std:.3f} | "
                   f"{tt_mean:.3f} Â± {tt_std:.3f} | "
                   f"{avg_size:.1f} |\n")
        
        f.write("\n## è¯¦ç»†æ•°æ®\n\n")
        
        for i, result in enumerate(results, 1):
            config = result["config"]
            f.write(f"### æµ‹è¯• {i}: {config['api']} - {config['text_type']}\n\n")
            
            f.write("| Run | é¦–å­—èŠ‚å»¶è¿Ÿ (s) | æ€»æ—¶é—´ (s) | æ–‡ä»¶å¤§å° (KB) | ååé‡ (Kbps) |\n")
            f.write("|-----|----------------|-----------|--------------|---------------|\n")
            
            for j, run in enumerate(result["results"], 1):
                f.write(f"| {j} | {run['first_byte_latency']:.3f} | "
                       f"{run['total_time']:.3f} | "
                       f"{run['total_bytes']/1024:.1f} | "
                       f"{run['throughput_kbps']:.1f} |\n")
            
            f.write(f"\n**å¹³å‡é¦–å­—èŠ‚å»¶è¿Ÿ**: {result['first_byte_latency']['mean']:.3f}s\n")
            f.write(f"**å¹³å‡æ€»æ—¶é—´**: {result['total_time']['mean']:.3f}s\n\n")
    
    print(f"âœ… æŠ¥å‘Šå·²ä¿å­˜åˆ°: {md_file}")

if __name__ == "__main__":
    run_benchmark()
