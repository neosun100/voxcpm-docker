#!/usr/bin/env python3
"""
Optimized OpenAI API Benchmark - WAV format only
å¯¹æ¯”ä¼˜åŒ–å‰åçš„æ€§èƒ½å·®å¼‚
"""
import requests
import time
import json
from pathlib import Path
import statistics

BASE_URL = "http://localhost:7861"
OUTPUT_DIR = Path("benchmark_optimized_results")
OUTPUT_DIR.mkdir(exist_ok=True)

TEST_TEXTS = {
    "short": "ä½ å¥½ï¼Œè¿™æ˜¯ä¸€ä¸ªç®€çŸ­çš„æµ‹è¯•ã€‚",
    "medium": "äººå·¥æ™ºèƒ½æŠ€æœ¯æ­£åœ¨å¿«é€Ÿå‘å±•ï¼Œè¯­éŸ³åˆæˆä½œä¸ºå…¶ä¸­çš„é‡è¦åˆ†æ”¯ï¼Œå·²ç»åœ¨å¤šä¸ªé¢†åŸŸå¾—åˆ°äº†å¹¿æ³›åº”ç”¨ã€‚VoxCPM æä¾›äº†é«˜è´¨é‡çš„è¯­éŸ³åˆæˆæœåŠ¡ã€‚",
    "long": "åœ¨å½“ä»Šæ•°å­—åŒ–æ—¶ä»£ï¼Œäººå·¥æ™ºèƒ½æŠ€æœ¯çš„å‘å±•æ—¥æ–°æœˆå¼‚ï¼Œå…¶ä¸­è¯­éŸ³åˆæˆæŠ€æœ¯ä½œä¸ºäººæœºäº¤äº’çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œæ­£åœ¨ç»å†ç€é©å‘½æ€§çš„å˜é©ã€‚ä»æ—©æœŸçš„æœºæ¢°å¼åˆæˆåˆ°ç°åœ¨çš„ç¥ç»ç½‘ç»œé©±åŠ¨çš„è‡ªç„¶è¯­éŸ³ç”Ÿæˆï¼ŒæŠ€æœ¯çš„è¿›æ­¥è®©æœºå™¨çš„å£°éŸ³è¶Šæ¥è¶Šæ¥è¿‘çœŸäººã€‚VoxCPM ä½œä¸ºæ–°ä¸€ä»£çš„è¯­éŸ³åˆæˆç³»ç»Ÿï¼Œé‡‡ç”¨äº†å…ˆè¿›çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡ã€è‡ªç„¶æµç•…çš„è¯­éŸ³ã€‚"
}

def test_api(text, api_type, format="wav", run_num=1):
    """æµ‹è¯• API"""
    print(f"\n{'='*60}")
    print(f"Run #{run_num}: {api_type}, {len(text)} chars, format={format}")
    print(f"{'='*60}")
    
    if api_type == "openai":
        url = f"{BASE_URL}/v1/audio/speech"
        payload = {
            "model": "tts-1",
            "input": text,
            "voice": "alloy",
            "response_format": format
        }
        response = requests.post(url, json=payload, stream=True, timeout=120)
    else:  # native
        url = f"{BASE_URL}/api/tts/stream"
        data = {
            "text": text,
            "voice_id": "default",
            "cfg_value": "2.0",
            "inference_timesteps": "5"
        }
        response = requests.post(url, data=data, stream=True, timeout=120)
    
    start_time = time.time()
    first_byte_time = None
    total_bytes = 0
    
    if response.status_code != 200:
        print(f"âŒ Error: {response.status_code}")
        return None
    
    for chunk in response.iter_content(chunk_size=8192):
        if chunk:
            if first_byte_time is None:
                first_byte_time = time.time()
                first_byte_latency = first_byte_time - start_time
                print(f"â±ï¸  é¦–å­—èŠ‚: {first_byte_latency:.3f}s")
            total_bytes += len(chunk)
    
    total_time = time.time() - start_time
    
    result = {
        "api_type": api_type,
        "format": format,
        "text_length": len(text),
        "first_byte_latency": first_byte_latency,
        "total_time": total_time,
        "total_bytes": total_bytes
    }
    
    print(f"âœ… å®Œæˆ! é¦–å­—èŠ‚: {first_byte_latency:.3f}s, æ€»æ—¶é—´: {total_time:.3f}s, å¤§å°: {total_bytes/1024:.1f}KB")
    return result

def run_benchmark():
    """è¿è¡Œä¼˜åŒ–åçš„ benchmark"""
    print("\n" + "ğŸ¯"*30)
    print("Optimized OpenAI API Benchmark - WAV Format")
    print("ğŸ¯"*30)
    
    all_results = []
    
    # æµ‹è¯•é…ç½®ï¼šå¯¹æ¯” OpenAI WAV vs Native
    configs = [
        {"api": "openai", "format": "wav", "text": "short", "runs": 5},
        {"api": "native", "format": "wav", "text": "short", "runs": 5},
        {"api": "openai", "format": "wav", "text": "medium", "runs": 5},
        {"api": "native", "format": "wav", "text": "medium", "runs": 5},
        {"api": "openai", "format": "wav", "text": "long", "runs": 5},
        {"api": "native", "format": "wav", "text": "long", "runs": 5},
    ]
    
    for config in configs:
        text = TEST_TEXTS[config["text"]]
        print(f"\n{'#'*60}")
        print(f"æµ‹è¯•: {config['api']} - {config['text']} ({len(text)} chars)")
        print(f"{'#'*60}")
        
        run_results = []
        for i in range(config["runs"]):
            time.sleep(1)
            result = test_api(text, config["api"], config["format"], i+1)
            if result:
                run_results.append(result)
        
        if run_results:
            fb_latencies = [r["first_byte_latency"] for r in run_results]
            total_times = [r["total_time"] for r in run_results]
            
            summary = {
                "config": config,
                "runs": len(run_results),
                "first_byte_latency": {
                    "mean": statistics.mean(fb_latencies),
                    "min": min(fb_latencies),
                    "max": max(fb_latencies),
                    "stdev": statistics.stdev(fb_latencies) if len(fb_latencies) > 1 else 0
                },
                "total_time": {
                    "mean": statistics.mean(total_times),
                    "min": min(total_times),
                    "max": max(total_times),
                    "stdev": statistics.stdev(total_times) if len(total_times) > 1 else 0
                },
                "results": run_results
            }
            
            all_results.append(summary)
            
            print(f"\nğŸ“Š ç»Ÿè®¡:")
            print(f"   é¦–å­—èŠ‚: {summary['first_byte_latency']['mean']:.3f}s Â± {summary['first_byte_latency']['stdev']:.3f}s")
            print(f"   æ€»æ—¶é—´: {summary['total_time']['mean']:.3f}s Â± {summary['total_time']['stdev']:.3f}s")
    
    # ä¿å­˜ç»“æœ
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    json_file = OUTPUT_DIR / f"benchmark_optimized_{timestamp}.json"
    
    with open(json_file, "w", encoding="utf-8") as f:
        json.dump(all_results, f, indent=2, ensure_ascii=False)
    
    # ç”ŸæˆæŠ¥å‘Š
    generate_report(all_results, timestamp)
    
    return all_results

def generate_report(results, timestamp):
    """ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š"""
    md_file = OUTPUT_DIR / f"benchmark_optimized_{timestamp}.md"
    
    with open(md_file, "w", encoding="utf-8") as f:
        f.write("# OpenAI API ä¼˜åŒ–åæ€§èƒ½æŠ¥å‘Š\n\n")
        f.write(f"**æµ‹è¯•æ—¶é—´**: {time.strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        f.write("**ä¼˜åŒ–å†…å®¹**: ä½¿ç”¨ WAV æ ¼å¼ï¼Œé¿å… MP3 ç¼–ç è½¬æ¢\n\n")
        
        f.write("## æ€§èƒ½å¯¹æ¯”\n\n")
        f.write("| API | æ–‡æœ¬ | é¦–å­—èŠ‚å»¶è¿Ÿ (s) | æ€»æ—¶é—´ (s) | æ–‡ä»¶å¤§å° (KB) |\n")
        f.write("|-----|------|----------------|-----------|-------------|\n")
        
        for result in results:
            config = result["config"]
            fb = result["first_byte_latency"]["mean"]
            fb_std = result["first_byte_latency"]["stdev"]
            tt = result["total_time"]["mean"]
            tt_std = result["total_time"]["stdev"]
            size = statistics.mean([r["total_bytes"]/1024 for r in result["results"]])
            
            f.write(f"| {config['api']} | {config['text']} | "
                   f"{fb:.3f} Â± {fb_std:.3f} | "
                   f"{tt:.3f} Â± {tt_std:.3f} | "
                   f"{size:.1f} |\n")
        
        # è®¡ç®—æ”¹è¿›
        f.write("\n## æ€§èƒ½æ”¹è¿›åˆ†æ\n\n")
        
        for i in range(0, len(results), 2):
            if i+1 < len(results):
                openai_result = results[i]
                native_result = results[i+1]
                
                text_type = openai_result["config"]["text"]
                
                openai_fb = openai_result["first_byte_latency"]["mean"]
                native_fb = native_result["first_byte_latency"]["mean"]
                fb_diff = ((openai_fb - native_fb) / native_fb) * 100
                
                openai_tt = openai_result["total_time"]["mean"]
                native_tt = native_result["total_time"]["mean"]
                tt_diff = ((openai_tt - native_tt) / native_tt) * 100
                
                f.write(f"### {text_type.upper()} æ–‡æœ¬\n\n")
                f.write(f"- **é¦–å­—èŠ‚å»¶è¿Ÿ**: OpenAI {openai_fb:.3f}s vs Native {native_fb:.3f}s ({fb_diff:+.1f}%)\n")
                f.write(f"- **æ€»æ—¶é—´**: OpenAI {openai_tt:.3f}s vs Native {native_tt:.3f}s ({tt_diff:+.1f}%)\n\n")
    
    print(f"\nâœ… æŠ¥å‘Šå·²ä¿å­˜: {md_file}")

if __name__ == "__main__":
    run_benchmark()
